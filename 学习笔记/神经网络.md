# 神经网络

- ## 构成<mark>[全链接网络]</mark>

![b04f06bc-9768-4b06-9012-8c0baad093f8](file:///D:/picture/Typedown/b04f06bc-9768-4b06-9012-8c0baad093f8.png)

        用〇表示神经元，用箭头表示它们的连接。此时，**在箭头上有权重**，这个权重和对应的神经元的值分别相乘，其和（严格地讲，是经过激活函数变换后的值）作为下一个神经元的输入。另外，此时还要**加上一个不受前一层的神经元影响的常数，这个常数称为偏置**。因为所有相邻的神经元之间都存在由箭头表示的连接，所以图 1-7 的神经网络称为全连接网络。

- ## 神经网络的推理

- #### 全连接数学定义

        **用(x1, x2)表示输入层的数据，用w11和w12表示权重，用b1表示偏置**。这样一来，图1-7中的隐藏层的**第1个神经元**就可以如下进行计算：

                                                     $ h1=x1w11+x2w21+b1$

        实际上，基于全连接层的变换可以通过矩阵乘积如下进行整理：                                     

 $  (h_1,h_2,h_3,h_4) = (x_1,x_2)( \begin{matrix} w_{11}& w_{12}&w_{13}&w_{14}\\w_{21} &w_{22}&w_{23}& w_{24}\end{matrix} )+(b1,b2,b3,b4)$

即：

                                                              $h=xW+b$

      **说明**:

            1. $x$的行数：这一个特征有多少个**样本数据**

                $x$的列数：第i和变量

            2. $W_{ij}$：表示第$i$个**输入**（特征）与第$j$个隐藏层之间的**权重**。

- #### 神经网络推理过程 **【正向传播】**

        1. 将输入层通过权重和偏置的计算得到隐藏层【Affine层求值】

        2. 对隐藏层使用**激活函数**，将全连接层的线性变换赋予 **“非线性“** 的效果——*可以增强神经网络的表现力*。【Sigmoid层进行变换】

        3. 将得到的变换后的隐藏再用全连接层做变换，得到输出。*行数表示隐藏层神经元的数量，列数表示输出层神经元的数量*



    此时，输出层的神经元数量与输入层的神经元的数量不同，维数改变了。

![388f6eb3-5238-4ea3-b5b5-d177caaa3703](file:///D:/picture/Typedown/388f6eb3-5238-4ea3-b5b5-d177caaa3703.png)


